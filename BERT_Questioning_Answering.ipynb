{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_Questioning_Answering.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOV9oKvrVMyMRCGvLuldykT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "452c58c7038c496c99b669e3c6e7ae7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b0315542e00047bdab90768a5f051576",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2b5295f35e1f4e50ace25b2b177480c1",
              "IPY_MODEL_e70ff1f4cb924c288a85d22c92bbef67"
            ]
          }
        },
        "b0315542e00047bdab90768a5f051576": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2b5295f35e1f4e50ace25b2b177480c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_75f43358d37343ec99ce46c992e72140",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_23e2a46eb53d48929f8a07e6d031e111"
          }
        },
        "e70ff1f4cb924c288a85d22c92bbef67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e123baf446e849c7a35df6f3153d41d7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [00:01&lt;00:00, 261B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6de2be282b824f90b45756ad9a32c807"
          }
        },
        "75f43358d37343ec99ce46c992e72140": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "23e2a46eb53d48929f8a07e6d031e111": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e123baf446e849c7a35df6f3153d41d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6de2be282b824f90b45756ad9a32c807": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3d82889a544d4a9db3e5816d21cf0e72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_93470aa393bd4f779ab09505d64e1669",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4bb1aeb44f7944d4aee6d353a587f525",
              "IPY_MODEL_bb94996f1dde45eb84f02b4ffd136b4d"
            ]
          }
        },
        "93470aa393bd4f779ab09505d64e1669": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4bb1aeb44f7944d4aee6d353a587f525": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2b1a2b001ee04d99b10df8aa4fe61796",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 536063208,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 536063208,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0c32ed44bfed4655a592d445c041f436"
          }
        },
        "bb94996f1dde45eb84f02b4ffd136b4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_840ba923d0654b76a1db0c93bf3e6bcc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 536M/536M [00:46&lt;00:00, 11.6MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_34ddda12bfb4460c91a45e1719a9005f"
          }
        },
        "2b1a2b001ee04d99b10df8aa4fe61796": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0c32ed44bfed4655a592d445c041f436": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "840ba923d0654b76a1db0c93bf3e6bcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "34ddda12bfb4460c91a45e1719a9005f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kjcoursera/NLP/blob/master/BERT_Questioning_Answering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKBnC8kusfII",
        "outputId": "f82e5a86-5962-4b27-b8cf-6e5bb6e3c6f0"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zw81Sd-J1FYl"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FZYcmZnsy18"
      },
      "source": [
        "#pip install transformers==2.5.1"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNiJ6f4ttjm7"
      },
      "source": [
        "import glob\n",
        "# Importing drive method from colab for accessing google drive\n",
        "from google.colab import drive"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQG4zI86tsbO",
        "outputId": "9b1c7a86-879d-4f40-d61f-3b565bd97469"
      },
      "source": [
        "# Mounting drive\n",
        "# This will require authentication : Follow the steps as guided\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCBBqnrRt71n",
        "outputId": "cd26454b-75df-419e-ca2d-b87b88af405e"
      },
      "source": [
        "!ls \"/content/drive/My Drive/Colab Notebooks/NLP/models\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "config.json  special_tokens_map.json  tokenizer_config.json  vocab.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSbR6XQos9zf"
      },
      "source": [
        "from transformers import *\n",
        "from transformers import AutoTokenizer, TFAutoModelForQuestionAnswering\n",
        "#from transformers import AutoModelForQuestionAnswering, TFAutoModelForQuestionAnswering"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCrfN4ZDtAqL"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/My Drive/Colab Notebooks/NLP/models\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKPxKCaGuq4w"
      },
      "source": [
        "passage = \"My name is Bob.\"\n",
        "question = \"What is my name?\""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqZCiQ-Vuvep"
      },
      "source": [
        "def prepare_bert_input(question, passage, tokenizer, max_seq_length=384):\n",
        "    \"\"\"\n",
        "    Prepare question and passage for input to BERT. \n",
        "\n",
        "    Args:\n",
        "        question (string): question string\n",
        "        passage (string): passage string where answer should lie\n",
        "        tokenizer (Tokenizer): used for transforming raw string input\n",
        "        max_seq_length (int): length of BERT input\n",
        "    \n",
        "    Returns:\n",
        "        input_ids (tf.Tensor): tensor of size (1, max_seq_length) which holds\n",
        "                               ids of tokens in input\n",
        "        input_mask (list): list of length max_seq_length of 1s and 0s with 1s\n",
        "                           in indices corresponding to input tokens, 0s in\n",
        "                           indices corresponding to padding\n",
        "        tokens (list): list of length of actual string tokens corresponding to input_ids\n",
        "    \"\"\"\n",
        "    # tokenize question\n",
        "    question_tokens = tokenizer.tokenize(question)\n",
        "    \n",
        "    # tokenize passage\n",
        "    passage_token = tokenizer.tokenize(passage)\n",
        "\n",
        "    # get special tokens \n",
        "    CLS = tokenizer.cls_token\n",
        "    SEP = tokenizer.sep_token\n",
        "    \n",
        "    # manipulate tokens to get input in correct form (not adding padding yet)\n",
        "    # CLS {question_tokens} SEP {answer_tokens} \n",
        "    # This should be a list of tokens\n",
        "    tokens = [CLS] + question_tokens + [SEP] + passage_token\n",
        "\n",
        "    \n",
        "    # Convert tokens into integer IDs.\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    \n",
        "    # Create an input mask which has integer 1 for each token in the 'tokens' list\n",
        "    input_mask = [1]*len(input_ids)\n",
        "\n",
        "    # pad input_ids with 0s until it is the max_seq_length\n",
        "    # Create padding for input_ids by creating a list of zeros [0,0,...0]\n",
        "    # Add the padding to input_ids so that its length equals max_seq_length\n",
        "    input_ids = input_ids + [0]*(max_seq_length - len(input_ids))\n",
        "    \n",
        "    # Do the same to pad the input_mask so its length is max_seq_length\n",
        "    input_mask = input_mask + [0]*(max_seq_length - len(input_mask))\n",
        "\n",
        "    # END CODE HERE\n",
        "\n",
        "    return tf.expand_dims(tf.convert_to_tensor(input_ids), 0), input_mask, tokens  "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tb5InTkBu2gX"
      },
      "source": [
        "input_ids, input_mask, tokens = prepare_bert_input(question, passage, tokenizer, max_seq_length=20)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0LHG5bEu49M"
      },
      "source": [
        "def get_span_from_scores(start_scores, end_scores, input_mask, verbose = False):\n",
        "    n = len(start_scores)\n",
        "    max_start_i = -1\n",
        "    max_end_j = -1\n",
        "    max_start_score = -np.inf\n",
        "    max_end_score = -np.inf\n",
        "    max_sum = -np.inf\n",
        "    for i in range(n):\n",
        "      for j in range(i,n):\n",
        "        if (input_mask[i] == input_mask[j] == 1):\n",
        "          #print(i,j,input_mask[i], start_scores[i]+ end_scores[j])\n",
        "          if (start_scores[i]+ end_scores[j]) > max_sum:\n",
        "            max_sum = start_scores[i] + end_scores[j]\n",
        "            \n",
        "            max_start_i = i\n",
        "            max_end_j = j\n",
        "            max_start_val = start_scores[i]\n",
        "            max_end_val = end_scores[j]\n",
        "            #print(i,j,input_mask[i],input_mask[j],max_start_val, max_sum ,'*')\n",
        "    if verbose:\n",
        "        print(f\"max start is at index i={max_start_i} and score {max_start_val}\")\n",
        "        print(f\"max end is at index i={max_end_j} and score {max_end_val}\")\n",
        "        print(f\"max start + max end sum of scores is {max_sum}\")\n",
        "    return max_start_i, max_end_j\n",
        "\n",
        "   "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LhGdXeLu68E",
        "outputId": "0bdfcac0-12ee-42fb-87b9-3b59a65c4d9e"
      },
      "source": [
        "start_scores = tf.convert_to_tensor([-1, 2, 0.4, -0.3, 0, 8, 10, 12], dtype=float)\n",
        "end_scores = tf.convert_to_tensor([5, 1, 1, 3, 4, 10, 10, 10], dtype=float)\n",
        "input_mask = [1, 1, 1, 1, 1, 0, 0, 0]\n",
        "\n",
        "start, end = get_span_from_scores(start_scores, end_scores, input_mask, verbose=True)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max start is at index i=1 and score 2.0\n",
            "max end is at index i=4 and score 4.0\n",
            "max start + max end sum of scores is 6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGuGRBoF0sBP",
        "outputId": "4e814a4a-4e3e-4e1b-deb7-27e3366c5097"
      },
      "source": [
        "# Test 2\n",
        "start_scores = tf.convert_to_tensor([0, 2, -1, 0.4, -0.3, 0, 8, 10, 12], dtype=float)\n",
        "end_scores = tf.convert_to_tensor([0, 5, 1, 1, 3, 4, 10, 10, 10], dtype=float)\n",
        "input_mask = [1, 1, 1, 1, 1, 0, 0, 0, 0 ]\n",
        "\n",
        "start, end = get_span_from_scores(start_scores, end_scores, input_mask, verbose=True)\n",
        "\n",
        "print(\"Expected: (1, 1) \\nReturned: ({}, {})\".format(start, end))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max start is at index i=1 and score 2.0\n",
            "max end is at index i=1 and score 5.0\n",
            "max start + max end sum of scores is 7.0\n",
            "Expected: (1, 1) \n",
            "Returned: (1, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUqHcNPTu9Uu"
      },
      "source": [
        "def construct_answer(tokens):\n",
        "  out_string = ' '.join(tokens)\n",
        "  out_string = out_string.replace(' ##','')\n",
        "  out_string = out_string.strip()\n",
        "  if '@' in tokens:\n",
        "    out_string = out_string.replace(' ', '')\n",
        "  return out_string\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTNOkq0F8vYk",
        "outputId": "441b3811-1c5e-438b-a0fb-06210777c368"
      },
      "source": [
        "tmp_tokens_1 = [' ## hello', 'how ', 'are ', 'you?      ']\n",
        "tmp_out_string_1 = construct_answer(tmp_tokens_1)\n",
        "\n",
        "print(f\"tmp_out_string_1: {tmp_out_string_1}, length {len(tmp_out_string_1)}\")\n",
        "\n",
        "\n",
        "tmp_tokens_2 = ['@',' ## hello', 'how ', 'are ', 'you?      ']\n",
        "tmp_out_string_2 = construct_answer(tmp_tokens_2)\n",
        "print(f\"tmp_out_string_2: {tmp_out_string_2}, length {len(tmp_out_string_2)}\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tmp_out_string_1: hello how  are  you?, length 20\n",
            "tmp_out_string_2: @hellohowareyou?, length 16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116,
          "referenced_widgets": [
            "452c58c7038c496c99b669e3c6e7ae7b",
            "b0315542e00047bdab90768a5f051576",
            "2b5295f35e1f4e50ace25b2b177480c1",
            "e70ff1f4cb924c288a85d22c92bbef67",
            "75f43358d37343ec99ce46c992e72140",
            "23e2a46eb53d48929f8a07e6d031e111",
            "e123baf446e849c7a35df6f3153d41d7",
            "6de2be282b824f90b45756ad9a32c807",
            "3d82889a544d4a9db3e5816d21cf0e72",
            "93470aa393bd4f779ab09505d64e1669",
            "4bb1aeb44f7944d4aee6d353a587f525",
            "bb94996f1dde45eb84f02b4ffd136b4d",
            "2b1a2b001ee04d99b10df8aa4fe61796",
            "0c32ed44bfed4655a592d445c041f436",
            "840ba923d0654b76a1db0c93bf3e6bcc",
            "34ddda12bfb4460c91a45e1719a9005f"
          ]
        },
        "id": "QVJ29bWe93kA",
        "outputId": "e03d166e-572b-4229-b11f-a0be5704db79"
      },
      "source": [
        "model = TFAutoModelForQuestionAnswering.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "452c58c7038c496c99b669e3c6e7ae7b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3d82889a544d4a9db3e5816d21cf0e72",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=536063208.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAsqXdlZPO5G"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
        "model = TFAutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZjE43oyGIcq"
      },
      "source": [
        "def get_model_answer(model, question, passage, tokenizer, max_seq_length=384):\n",
        "    \"\"\"\n",
        "    Identify answer in passage for a given question using BERT. \n",
        "\n",
        "    Args:\n",
        "        model (Model): pretrained Bert model which we'll use to answer questions\n",
        "        question (string): question string\n",
        "        passage (string): passage string\n",
        "        tokenizer (Tokenizer): used for preprocessing of input\n",
        "        max_seq_length (int): length of input for model\n",
        "        \n",
        "    Returns:\n",
        "        answer (string): answer to input question according to model\n",
        "    \"\"\" \n",
        "    # prepare input: use the function prepare_bert_input\n",
        "    input_ids, input_mask, tokens = prepare_bert_input(question, passage, tokenizer, max_seq_length)\n",
        "    \n",
        "    # get scores for start of answer and end of answer\n",
        "    # use the model returned by TFAutoModelForQuestionAnswering.from_pretrained(\"./models\")\n",
        "    # pass in in the input ids that are returned by prepare_bert_input\n",
        "    start_scores, end_scores = model(input_ids)\n",
        "    \n",
        "    # start_scores and end_scores will be tensors of shape [1,max_seq_length]\n",
        "    # To pass these into get_span_from_scores function, \n",
        "    # take the value at index 0 to get a tensor of shape [max_seq_length]\n",
        "    start_scores = start_scores[0]\n",
        "    end_scores = end_scores[0]\n",
        "    \n",
        "    # using scores, get most likely answer\n",
        "    # use the get_span_from_scores function\n",
        "    span_start, span_end = get_span_from_scores(start_scores, end_scores, input_mask)\n",
        "    \n",
        "    # Using array indexing to get the tokens from the span start to span end (including the span_end)\n",
        "    answer_tokens = tokens[span_start:span_end+1]\n",
        "    \n",
        "    # Combine the tokens into a single string and perform post-processing\n",
        "    # use construct_answer\n",
        "    answer = construct_answer(answer_tokens)\n",
        "    \n",
        "    return answer"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptvKGUvMQI5F",
        "outputId": "e7179681-71af-42bd-d853-7acbfd0af3b8"
      },
      "source": [
        "passage = \"Computational complexity theory is a branch of the theory \\\n",
        "           of computation in theoretical computer science that focuses \\\n",
        "           on classifying computational problems according to their inherent \\\n",
        "           difficulty, and relating those classes to each other. A computational \\\n",
        "           problem is understood to be a task that is in principle amenable to \\\n",
        "           being solved by a computer, which is equivalent to stating that the \\\n",
        "           problem may be solved by mechanical application of mathematical steps, \\\n",
        "           such as an algorithm.\"\n",
        "\n",
        "question = \"What branch of theoretical computer science deals with broadly \\\n",
        "            classifying computational problems by difficulty and class of relationship?\"\n",
        "\n",
        "print(\"Output: {}\".format(get_model_answer(model, question, passage, tokenizer)))\n",
        "print(\"Expected: Computational complexity theory\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output: computational complexity theory\n",
            "Expected: Computational complexity theory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeonQ7mSGV98",
        "outputId": "28e78887-2241-4b13-ce58-6ac94fdee727"
      },
      "source": [
        "passage = \"The word pharmacy is derived from its root word pharma which was a term used since \\\n",
        "           the 15th–17th centuries. However, the original Greek roots from pharmakos imply sorcery \\\n",
        "           or even poison. In addition to pharma responsibilities, the pharma offered general medical \\\n",
        "           advice and a range of services that are now performed solely by other specialist practitioners, \\\n",
        "           such as surgery and midwifery. The pharma (as it was referred to) often operated through a \\\n",
        "           retail shop which, in addition to ingredients for medicines, sold tobacco and patent medicines. \\\n",
        "           Often the place that did this was called an apothecary and several languages have this as the \\\n",
        "           dominant term, though their practices are more akin to a modern pharmacy, in English the term \\\n",
        "           apothecary would today be seen as outdated or only approproriate if herbal remedies were on offer \\\n",
        "           to a large extent. The pharmas also used many other herbs not listed. The Greek word Pharmakeia \\\n",
        "           (Greek: φαρμακεία) derives from pharmakon (φάρμακον), meaning 'drug', 'medicine' (or 'poison').\"\n",
        "\n",
        "question = \"What word is the word pharmacy taken from?\"\n",
        "\n",
        "print(\"Output: {}\".format(get_model_answer(model, question, passage, tokenizer)))\n",
        "print(\"Expected: pharma\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output: pharma\n",
            "Expected: pharma\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkDcIAfXQfAG",
        "outputId": "b109a4b0-a817-4961-a191-af321639c6c3"
      },
      "source": [
        "passage = \"Abnormal echocardiogram findings and followup. Shortness of breath, congestive heart failure, \\\n",
        "           and valvular insufficiency. The patient complains of shortness of breath, which is worsening. \\\n",
        "           The patient underwent an echocardiogram, which shows severe mitral regurgitation and also large \\\n",
        "           pleural effusion. The patient is an 86-year-old female admitted for evaluation of abdominal pain \\\n",
        "           and bloody stools. The patient has colitis and also diverticulitis, undergoing treatment. \\\n",
        "           During the hospitalization, the patient complains of shortness of breath, which is worsening. \\\n",
        "           The patient underwent an echocardiogram, which shows severe mitral regurgitation and also large \\\n",
        "           pleural effusion. This consultation is for further evaluation in this regard. As per the patient, \\\n",
        "           she is an 86-year-old female, has limited activity level. She has been having shortness of breath \\\n",
        "           for many years. She also was told that she has a heart murmur, which was not followed through \\\n",
        "           on a regular basis.\"\n",
        "\n",
        "q1 = \"How old is the patient?\"\n",
        "q2 = \"Does the patient have any complaints?\"\n",
        "q3 = \"What is the reason for this consultation?\"\n",
        "q4 = \"What does her echocardiogram show?\"\n",
        "q5 = \"What other symptoms does the patient have?\"\n",
        "\n",
        "\n",
        "questions = [q1, q2, q3, q4, q5]\n",
        "\n",
        "for i, q in enumerate(questions):\n",
        "    print(\"Question {}: {}\".format(i+1, q))\n",
        "    print()\n",
        "    print(\"Answer: {}\".format(get_model_answer(model, q, passage, tokenizer)))\n",
        "    print()\n",
        "    print()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question 1: How old is the patient?\n",
            "\n",
            "Answer: 86 - year - old\n",
            "\n",
            "\n",
            "Question 2: Does the patient have any complaints?\n",
            "\n",
            "Answer: the patient complains of shortness of breath\n",
            "\n",
            "\n",
            "Question 3: What is the reason for this consultation?\n",
            "\n",
            "Answer: shortness of breath , which is worsening . the patient underwent an echocardiogram , which shows severe mitral regurgitation and also large pleural effusion . the patient is an 86 - year - old female admitted for evaluation of abdominal pain and bloody stools . the patient has colitis and also diverticulitis , undergoing treatment . during the hospitalization , the patient complains of shortness of breath , which is worsening . the patient underwent an echocardiogram , which shows severe mitral regurgitation and also large pleural effusion . this consultation is for further evaluation in this regard . as per the patient , she is an 86 - year - old female , has limited activity level\n",
            "\n",
            "\n",
            "Question 4: What does her echocardiogram show?\n",
            "\n",
            "Answer: severe mitral regurgitation and also large pleural effusion\n",
            "\n",
            "\n",
            "Question 5: What other symptoms does the patient have?\n",
            "\n",
            "Answer: [CLS] what other symptoms does the patient have ? [SEP] abnormal echocardiogram findings and followup . shortness of breath , congestive heart failure , and valvular insufficiency\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7JTPUVZMtog",
        "outputId": "a89f0fae-3214-48f3-8460-a99944ed045e"
      },
      "source": [
        "from transformers import AutoTokenizer, TFAutoModelForQuestionAnswering\n",
        "import tensorflow as tf\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
        "model = TFAutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
        "\n",
        "text = r\"\"\"\n",
        "🤗 Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\n",
        "architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet...) for Natural Language Understanding (NLU) and Natural\n",
        "Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\n",
        "TensorFlow 2.0 and PyTorch.\n",
        "\"\"\"\n",
        "\n",
        "questions = [\n",
        "    \"How many pretrained models are available in Transformers?\",\n",
        "    \"What does Transformers provide?\",\n",
        "    \"Transformers provides interoperability between which frameworks?\",\n",
        "]\n",
        "\n",
        "for question in questions:\n",
        "    inputs = tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors=\"tf\")\n",
        "    input_ids = inputs[\"input_ids\"].numpy()[0]\n",
        "\n",
        "    text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "    answer_start_scores, answer_end_scores = model(inputs)\n",
        "\n",
        "    answer_start = tf.argmax(\n",
        "        answer_start_scores, axis=1\n",
        "    ).numpy()[0]  # Get the most likely beginning of answer with the argmax of the score\n",
        "    answer_end = (\n",
        "        tf.argmax(answer_end_scores, axis=1) + 1\n",
        "    ).numpy()[0]  # Get the most likely end of answer with the argmax of the score\n",
        "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
        "\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Answer: {answer}\\n\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question: How many pretrained models are available in Transformers?\n",
            "Answer: over 32 +\n",
            "\n",
            "Question: What does Transformers provide?\n",
            "Answer: general - purpose architectures\n",
            "\n",
            "Question: Transformers provides interoperability between which frameworks?\n",
            "Answer: tensorflow 2 . 0 and pytorch\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}